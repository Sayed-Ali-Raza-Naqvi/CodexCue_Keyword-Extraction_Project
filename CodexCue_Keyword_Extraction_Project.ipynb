{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuMCKShh1BwQLh46nZ1jGX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayed-Ali-Raza-Naqvi/CodexCue_Keyword-Extraction_Project/blob/main/CodexCue_Keyword_Extraction_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Extraction"
      ],
      "metadata": {
        "id": "-jnnZI8Wykxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "brCgdh1ay0gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_csv('/content/papers.csv', engine='python', on_bad_lines='skip', quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error parsing CSV file: {e}\")"
      ],
      "metadata": {
        "id": "IPsq9GYky2em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "uETFp8Vry7AE",
        "outputId": "40f51a3d-688d-48ee-b852-19723b4d9276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      id       year  \\\n",
              "23846  pruning weights or eliminating biases. Vhcn a ...   vd }[~:1   \n",
              "12751                                               4253       2011   \n",
              "9088                                                2315       2002   \n",
              "14542                                               4964       2013   \n",
              "9941                                                2786       2005   \n",
              "\n",
              "                                                   title event_type  \\\n",
              "23846                                               None       None   \n",
              "12751  Signal Estimation Under Random Time-Warpings a...        NaN   \n",
              "9088                     Bayesian Image Super-Resolution        NaN   \n",
              "14542  Projecting Ising Model Parameters for Fast Mixing     Poster   \n",
              "9941   Oblivious Equilibrium: A Mean Field Approximat...        NaN   \n",
              "\n",
              "                                                pdf_name  \\\n",
              "23846                                               None   \n",
              "12751  4253-signal-estimation-under-random-time-warpi...   \n",
              "9088            2315-bayesian-image-super-resolution.pdf   \n",
              "14542  4964-projecting-ising-model-parameters-for-fas...   \n",
              "9941   2786-oblivious-equilibrium-a-mean-field-approx...   \n",
              "\n",
              "                                                abstract  \\\n",
              "23846                                               None   \n",
              "12751  While signal estimation under random amplitude...   \n",
              "9088                                    Abstract Missing   \n",
              "14542  Inference in general Ising models is difficult...   \n",
              "9941                                    Abstract Missing   \n",
              "\n",
              "                                              paper_text  \n",
              "23846                                               None  \n",
              "12751  Signal Estimation Under Random Time-Warpings\\n...  \n",
              "9088   Bayesian Image Super-Resolution\\n\\nMichael E. ...  \n",
              "14542  Projecting Ising Model Parameters for Fast Mix...  \n",
              "9941   Oblivious Equilibrium: A Mean Field\\nApproxima...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df731d64-0693-4cbe-b574-7637c8d7234a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23846</th>\n",
              "      <td>pruning weights or eliminating biases. Vhcn a ...</td>\n",
              "      <td>vd }[~:1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12751</th>\n",
              "      <td>4253</td>\n",
              "      <td>2011</td>\n",
              "      <td>Signal Estimation Under Random Time-Warpings a...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4253-signal-estimation-under-random-time-warpi...</td>\n",
              "      <td>While signal estimation under random amplitude...</td>\n",
              "      <td>Signal Estimation Under Random Time-Warpings\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>2315</td>\n",
              "      <td>2002</td>\n",
              "      <td>Bayesian Image Super-Resolution</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2315-bayesian-image-super-resolution.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Image Super-Resolution\\n\\nMichael E. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14542</th>\n",
              "      <td>4964</td>\n",
              "      <td>2013</td>\n",
              "      <td>Projecting Ising Model Parameters for Fast Mixing</td>\n",
              "      <td>Poster</td>\n",
              "      <td>4964-projecting-ising-model-parameters-for-fas...</td>\n",
              "      <td>Inference in general Ising models is difficult...</td>\n",
              "      <td>Projecting Ising Model Parameters for Fast Mix...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9941</th>\n",
              "      <td>2786</td>\n",
              "      <td>2005</td>\n",
              "      <td>Oblivious Equilibrium: A Mean Field Approximat...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2786-oblivious-equilibrium-a-mean-field-approx...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Oblivious Equilibrium: A Mean Field\\nApproxima...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df731d64-0693-4cbe-b574-7637c8d7234a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df731d64-0693-4cbe-b574-7637c8d7234a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df731d64-0693-4cbe-b574-7637c8d7234a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a750b3e1-8bcd-49c5-afe7-ccc61a64bbdd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a750b3e1-8bcd-49c5-afe7-ccc61a64bbdd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a750b3e1-8bcd-49c5-afe7-ccc61a64bbdd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"4253\",\n          \"2786\",\n          \"2315\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2011\",\n          \"2005\",\n          \"2002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Bayesian Image Super-Resolution\",\n          \"Oblivious Equilibrium: A Mean Field Approximation for Large-Scale Dynamic Games\",\n          \"Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2315-bayesian-image-super-resolution.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"While signal estimation under random amplitudes, phase shifts, and additive noise is studied frequently, the problem of estimating a deterministic signal under random time-warpings has been relatively unexplored. We present a novel framework for estimating the unknown signal that utilizes the action of the warping group to form an equivalence relation between signals. First, we derive an estimator for the equivalence class of the unknown signal using the notion of Karcher mean on the quotient space of equivalence classes. This step requires the use of Fisher-Rao Riemannian metric  and a square-root representation of signals to enable computations of distances and means under this metric. Then, we define a notion of the center of a class and show that the center of the estimated class is a consistent estimator of the underlying unknown signal. This estimation algorithm has many applications: (1)registration/alignment of functional data, (2) separation of phase/amplitude components of functional data, (3) joint demodulation and carrier estimation, and (4) sparse modeling of functional data. Here we demonstrate only (1) and (2):  Given signals are temporally aligned using nonlinear warpings and, thus, separated into their phase and amplitude components. The proposed method for signal alignment is shown to have state of the art performance using Berkeley growth, handwritten signatures, and neuroscience spike train data.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Bayesian Image Super-Resolution\\n\\nMichael E. Tipping and Christopher M. Bishop\\nMicrosoft Research\\nCambridge, CB3 OFB, U.K.\\n{ mtipping, cmbishop} @microsoft.com\\nhttp://research.microsoft.com/ \\\"-'{ mtipping,cmbishop}\\n\\nAbstract\\nThe extraction of a single high-quality image from a set of lowresolution images is an important problem which arises in fields\\nsuch as remote sensing, surveillance, medical imaging and the extraction of still images from video. Typical approaches are based\\non the use of cross-correlation to register the images followed by\\nthe inversion of the transformation from the unknown high resolution image to the observed low resolution images, using regularization to resolve the ill-posed nature of the inversion process. In\\nthis paper we develop a Bayesian treatment of the super-resolution\\nproblem in which the likelihood function for the image registration parameters is based on a marginalization over the unknown\\nhigh-resolution image. This approach allows us to estimate the\\nunknown point spread function, and is rendered tractable through\\nthe introduction of a Gaussian process prior over images. Results\\nindicate a significant improvement over techniques based on MAP\\n(maximum a-posteriori) point optimization of the high resolution\\nimage and associated registration parameters.\\n\\n1\\n\\nIntroduction\\n\\nThe task in super-resolution is to combine a set of low resolution images of the\\nsame scene in order to obtain a single image of higher resolution. Provided the\\nindividual low resolution images have sub-pixel displacements relative to each other,\\nit is possible to extract high frequency details of the scene well beyond the Nyquist\\nlimit of the individual source images.\\nIdeally the low resolution images would differ only through small (sub-pixel) translations, and would be otherwise identical. In practice, the transformations may be\\nmore substantial and involve rotations or more complex geometric distortions. In\\naddition the scene itself may change, for instance if the source images are successive frames in a video sequence. Here we focus attention on static scenes in which\\nthe transformations relating the source images correspond to translations and rotations, such as can be obtained by taking several images in succession using a hand\\nheld digital camera. Our approach is readily extended to more general projective\\ntransformations if desired. Larger changes in camera position or orientation can be\\n\\n\\fhandled using techniques of robust feature matching, constrained by the epipolar\\ngeometry, but such sophistication is unnecessary in the present context.\\nMost previous approaches, for example [1, 2, 3], perform an initial registration of\\nthe low resolution images with respect to each other, and then keep this registration\\nfixed. They then formulate probabilistic models of the image generation process,\\nand use maximum likelihood to determine the pixel intensities in the high resolution\\nimage. A more convincing approach [4] is to determine simultaneously both the low\\nresolution image registration parameters and the pixel values of the high resolution\\nimage, again through maximum likelihood.\\nAn obvious difficulty of these techniques is that if the high resolution image has too\\nfew pixels then not all of the available high frequency information is extracted from\\nthe observed images, whereas if it has too many pixels the maximum likelihood\\nsolution becomes ill conditioned. This is typically resolved by the introduction of\\npenalty terms to regularize the maximum likelihood solution, where the regularization coefficients may be set by cross-validation. The regularization terms are\\noften motivated in terms of a prior distribution over the high resolution image,\\nin which case the solution can be interpreted as a MAP (maximum a-posteriori)\\noptimization.\\nBaker and Kanade [5] have tried to improve the performance of super-resolution\\nalgorithms by developing domain-specific image priors, applicable to faces or text\\nfor example, which are learned from data. In this case the algorithm is effectively\\nhallucinating perceptually plausible high frequency features. Here we focus on general purpose algorithms applicable to any natural image, for which the prior encodes\\nonly high level information such as the correlation of nearby pixels.\\nThe key development in this paper, which distinguishes it from previous approaches,\\nis the use of Bayesian, rather than simply MAP, techniques by marginalizing over\\nthe unknown high resolution image in order to determine the low resolution image\\nregistration parameters. Our formulation also allows the choice of continuous values\\nfor the up-sampling process, as well the shift and rotation parameters governing the\\nimage registration.\\nThe generative process by which the high resolution image is smoothed to obtain a\\nlow resolution image is described by a point spread function (PSF). It has often been\\nassumed that the point spread function is known in advance, which is unrealistic.\\nSome authors [3] have estimated the PSF in advance using only the low resolution\\nimage data, and then kept this estimate fixed while extracting the high resolution\\nimage. A key advantage of our Bayesian marginalization is that it allows us to\\ndetermine the point spread function alongside both the registration parameters and\\nthe high resolution image in a single, coherent inference framework.\\nAs we show later, if we attempt to determine the PSF as well as the registration\\nparameters and the high resolution image by joint optimization, we obtain highly biased (over-fitted) results. By marginalizing over the unknown high resolution image\\nwe are able to determine the PSF and the registration parameters accurately, and\\nthereby reconstruct the high resolution image with subjectively very good quality.\\n\\n2\\n\\nBayesian Super-resolution\\n\\nSuppose we are given K low-resolution intensity images (the extension to 3-colour\\nimages is straightforward). We shall find it convenient notationally to represent\\nthe images as vectors y(k) of length M , where k = 1, ... , K, obtained by raster\\nscanning the pixels of the images. Each image is shifted and rotated relative to a\\n\\n\\freference image which we shall arbitrarily take to be y(1). The shifts are described\\nby 2-dimensional vectors Sk, and the rotations are described by angles Ok.\\nThe goal is to infer the underlying scene from which the low resolution images are\\ngenerated. We represent this scene by a single high-resolution image, which we\\nagain denote by a raster-scan vector x whose length is N ? M.\\nOur approach is based on a generative model for the observed low resolution images,\\ncomprising a prior over the high resolution image together with an observation\\nmodel describing the process by which a low resolution image is obtained from the\\nhigh resolution one.\\nIt should be emphasized that the real scene which we are trying to infer has effectively an infinite resolution, and that its description as a pixellated image is a\\ncomputational artefact. In particular if we take the number N of pixels in this image\\nto be large the inference algorithm should remain well behaved. This is not the case\\nwith maximum likelihood approaches in which the value of N must be limited to\\navoid ill-conditioning. In our approach, if N is large the correlation of neighbouring\\npixels is determined primarily by the prior, and the value of N is limited only by\\nthe computational cost of working with large numbers of high resolution pixels.\\nWe represent the prior over the high resolution image by a Gaussian process\\n\\np(x) = N(xIO, Zx)\\n\\n(1)\\n\\nwhere the covariance matrix Zx is chosen to be of the form\\n\\nZx(i , j) = Aexp {_llvi\\n\\n~2VjI12}.\\n\\n(2)\\n\\nHere Vi denotes the spatial position in the 2-dimensional image space of pixel i, the\\ncoefficient A measures the 'strength' of the prior, and r defines the correlation length\\nscale. Since we take Zx to be a fixed matrix, it is straightforward to use a different\\nfunctional form for Zx if desired. It should be noted that in our image representation\\nthe pixel intensity values lie in the range (-0.5,0.5), and so in principle a Gaussian\\nprocess prior is inappropriate 1 . In practice we have found that this causes little\\ndifficulty, and in Section 4 we discuss how a more appropriate distribution could be\\nused.\\nThe low resolution images are assumed to be generated from the high resolution\\nimage by first applying a shift and a rotation, then convolving with some point\\nspread function, and finally downsampling to the lower resolution. This is expressed\\nthrough the transformation equation\\n(3)\\nwhere ?(k) is a vector of independent Gaussian random variables ?i ~ N(O, /3-1),\\nwith zero mean and precision (inverse variance) /3, representing noise terms intended to model the camera noise as well as to capture any discrepancy between\\nour generative model and the observed data.\\nThe transformation matrix W(k) in (3) is given by a point spread function which\\ncaptures the down-sampling process and which we again take to have a 'Gaussian'\\nform\\n\\n(4)\\nlNote that the established work we have referenced, where a Gaussian prior or quadratic\\nregularlizer is utilised, also overlooks the bounded nature of the pixel space.\\n\\n\\fwith\\n(5)\\n\\nwhere j = 1, ... M and i = 1, ... , N. Here \\\"( represents the 'width' of the point\\nspread function, and we shall treat \\\"( as an unknown parameter to be determined\\nfrom the data. Note that our approach generalizes readily to any other form of\\npoint spread function, possibly containing several unknown parameters, provided it\\nis differentiable with respect to those parameters.\\nIn (5) the vector U)k) is the centre of the PSF and is dependent on the shift and\\nrotation of the low resolution image. We choose a parameterization in which the\\ncentre of rotation coincides with the centre v of the image, so that\\nU)k)\\n\\nwhere\\n\\nR(k)\\n\\n=\\n\\nR(k)(Vj -\\n\\nv) + v + Sk\\n\\n(6)\\n\\nis the rotation matrix\\nR\\n\\n(k)\\n\\n=\\n\\n(\\n\\ncosB k\\n\\n(7)\\n\\n_ sinB k\\n\\nWe can now write down the likelihood function in the form\\n(8)\\n\\nAssuming the images are generated independently from the model, we can then\\nwrite the posterior distribution over the high resolution image in the form\\n\\n(9)\\n(10)\\n\\nwith\\n\\nE~ [z;' +fi (~W(WW('))\\nJ.L\\n\\n= (3~\\n\\n(~W(k)T y(k)) .\\n\\nr,\\n\\n(11)\\n\\n(12)\\n\\nThus the posterior distribution over the high resolution image is again a Gaussian\\nprocess.\\nIf we knew the registration parameters {Sk' Bk }, as well as the PSF width parameter\\n,,(, then we could simply take the mean J.L (which is also the maximum) of the\\nposterior distribution to be our super-resolved image. However, the registration\\nparameters are unknown. Previous approaches have either performed a preliminary\\nregistration of the low resolution images against each other and then fixed the\\nregistration while determining the high resolution image, or else have maximized\\nthe posterior distribution (9) jointly with respect to the high resolution image x and\\nthe registration parameters (which we refer to as the 'MAP' approach). Neither\\napproach takes account of the uncertainty in determining the high resolution image\\nand the consequential effects on the optimization of the registration parameters.\\n\\n\\fHere we adopt a Bayesian approach by marginalizing out the unknown high resolution image. This gives the marginal likelihood function for the low resolution\\nimages in the form\\n\\n(13)\\nwhere\\n\\n(14)\\nand y and Ware the vector and matrix of stacked y(k) and W(k) respectively. Using\\nsome standard matrix manipulations we can rewrite the marginal likelihood in the\\nform\\n\\n10gp(YI {Sk' e k }, I ) =\\n\\n-\\\"21\\n\\n[\\n\\n,B\\n\\n2..: Ily(k) K\\n\\nW(k) J.L112 + J.L TZ;l J.L\\n\\nk=l\\n\\n+logIZxl-IOgl~I-KMIOg,B].\\n\\n(15)\\n\\nWe now wish to optimize this marginal likelihood with respect to the parameters\\n{sk,ed'I' and to do this we have compared two approaches. The first is to use\\nthe expectation-maximization (EM) algorithm. In the E-step we evaluate the posterior distribution over the high resolution image given by (10) . In the M-step\\nwe maximize the expectation over x of the log of the complete data likelihood\\np(y,xl{sk,ed'l) obtained from the product of the prior (1) and the likelihood (8).\\nThis maximization is done using the scaled conjugate gradients algorithm (SeG)\\n[6]. The second approach is to maximize the marginal likelihood (15) directly using\\nSeG. Empirically we find that direct optimization is faster than EM, and so has\\nbeen used to obtain the results reported in this paper.\\nSince in (15) we must compute ~, which is N x N, in practice we optimize the\\nshift, rotation and PSF width parameters based on an appropriately-sized subset\\nof the image only. The complete high resolution image is then found as the mode\\nof the full posterior distribution, obtained iteratively by maximizing the numerator\\nin (9), again using SeG optimization.\\n\\n3\\n\\nResults\\n\\nIn order to evaluate our approach we first apply it to a set of low resolution images\\nsynthetically down-sampled (by a linear scaling of 4 to 1, or 16 pixels to 1) from a\\nknown high-resolution image as follows. For each image we wish to generate we first\\napply a shift drawn from a uniform distribution over the interval (-2,2) in units\\nof high resolution pixels (larger shifts could in principle be reduced to this level\\nby pre-registering the low resolution images against each other) and then apply a\\nrotation drawn uniformly over the interval (-4,4) in units of degrees. Finally we\\ndetermine the value at each pixel of the low resolution image by convolution of the\\noriginal image with the point spread function (centred on the low resolution pixel),\\nwith width parameter 1 = 2.0. From a high-resolution image of 384 x 256 we chose\\nto use a set of 16 images of resolution 96 x 64.\\nIn order to limit the computational cost we use patches from the centre of the low\\nresolution image of size 9 x 9 in order to determine the values of the shift, rotation\\nand PSF width parameters. We set the resolution of the super-resolved image to\\nhave 16 times as many pixels as the low resolution images which, allowing for shifts\\nand the support of the point spread function, gives N = 50 x 50. The Gaussian\\nprocess prior is chosen to have width parameter r = 1.0, variance parameter A =\\n\\n\\f0.04, and the noise process is given a standard deviation of 0.05. Note that these\\nvalues can be set sensibly a priori and need not be tuned to the data.\\nThe scaled conjugate gradient optimization is initialised by setting the shift and\\nrotation parameters equal to zero, while the PSF width \\\"( is initialized to 4.0 since\\nthis is the upsampling factor we have chosen between low resolution and superresolved images. We first optimize only the shifts, then we optimize both shifts\\nand rotations, and finally we optimize shifts, rotations and PSF width, in each case\\nrunning until a suitable convergence tolerance is reached.\\n\\nIn Figure l(a) we show the original image, together with an example low resolution\\nimage in Figure l(b). Figure l(c) shows the super-resolved image obtained using our\\nBayesian approach. We see that the super-resolved image is of dramatically better\\nquality than the low resolution images from which it is inferred. The converged\\nvalue for the PSF width parameter is \\\"( = 1.94, close to the true value 2.0.\\n\\nFigure 1: Example using synthetically generated data showing (top left) the\\noriginal image, (top right) an example low resolution image and (bottom left)\\nthe inferred super-resolved image. Also shown, in (bottom right), is a comparison super-resolved image obtained by joint optimization with respect to\\nthe super-resolved image and the parameters, demonstrating the significanly\\npoorer result.\\n\\nNotice that there are some small edge effects in the super-resolved image arising from\\nthe fact that these pixels only receive evidence from a subset of the low resolution\\nimages due to the image shifts. Thus pixels near the edge of the high resolution\\nimage are determined primarily by the prior.\\n\\n\\fFor comparison we show, in Figure l(d), the corresponding super-resolved image\\nobtained by performing a MAP optimization with respect to the high resolution\\nimage. This is of significantly poorer quality than that obtained from our Bayesian\\napproach. The converged value for t he PSF width in this case is '\\\"Y = 0.43 indicating\\nsevere over-fitting.\\nIn Figure 2 we show plots of the true and estimated values for the shift and rotation\\nparameters using our Bayesian approach and also using MAP optimization. Again\\nwe see the severe over-fitting resulting from joint optimization, and the significantly\\nbetter results obtained from the Bayesian approach.\\n\\n(a) Shift estimation\\n\\n(b) Rotation estimation\\n\\n2.51.======;-~-~-~1\\n\\n2\\n1.5\\n\\ntruth\\n~\\nBayesian\\n'--l::,.\\n__\\nM_A_P_____\\n\\nI\\n\\nx\\n0\\n\\n1.8 r-;:::::::::::::::::====;--~--~I\\nBayesian\\n1.6 _\\nMAP\\n_\\n\\n~\\n\\n1\\n\\n1\\n\\n~\\ng'1.4\\n-0\\n\\n;:\\n..c\\n\\n(Jl\\n\\n::- 1.2\\n\\ne\\n\\n0.5\\n\\nCii\\n\\n~\\n0\\ntQ) -0.5\\n\\nc\\n\\n:2ctl 0.8\\n\\ne\\n\\n>\\n\\n-1\\n\\n0.6\\n\\nQ)\\n\\n~ 0.4\\n\\n-1.5\\n\\no(Jl\\n\\n-2\\n\\n-2.5 '---~-~--~--~-~----'\\n-2\\n\\n-1\\n\\n0\\n\\n1\\n\\nhorizontal shift\\n\\n2\\n\\n~ 0.: L.........IJIIL...IL..UI'-\\\"!....H...IL.-L..II:LII..oL..ll...II.lL..H..JIO!....aL..J\\nII\\no\\n\\n5\\n\\n10\\n\\n15\\n\\nlow-resolution imaae index\\n\\nFigure 2: (a) Plots of the true shifts for the synthetic data, together with the\\nestimated values obtained by optimization of the marginal likelihood in our\\nBayesian framework and for comparison the corresponding estimates obtained\\nby joint optimization with respect to registration parameters and the high\\nresolution image. (b) Comparison of the errors in determining the rotation\\nparameters for both Bayesian and MAP approaches.\\n\\nFinally, we apply our technique to a set of images obtained by taking 16 frames using\\na hand held digital camera in 'multi-shot' mode (press and hold the shutter release)\\nwhich takes about 12 seconds. An example image, together with the super-resolved\\nimage obtained using our Bayesian algorithm, is shown in Figure 3.\\n\\n4\\n\\nDiscussion\\n\\nIn this paper we have proposed a new approach to the problem of image superresolution, based on a marginalization over the unknown high resolution image using\\na Gaussian process prior. Our results demonstrate a worthwhile improvement over\\nprevious approaches based on MAP estimation, including the ability to estimate\\nparameters of the point spread function.\\nOne potential application our technique is the extraction of high resolution images\\nfrom video sequences. In this case it will be necessary to take account of motion\\nblur, as well as the registration, for example by tracking moving objects through\\nthe successive frames [7].\\n\\n\\f(a) Low-resolution image (1 of 16)\\n\\n(b) 4x Super-resolved image (Bayesian)\\n\\nFigure 3: Application to real data showing in (a) one of the 16 captured in\\nsuccession usind a hand held camera of a doorway with nearby printed sign.\\nImage (b) shows the final image obtained from our Bayesian super-resolution\\nalgorithm.\\n\\nFinally, having seen the advantages of marginalizing with respect to the high resolution image, we can extend this approach to a fully Bayesian one based on Markov\\nchain Monte Carlo sampling over all unknown parameters in the model. Since our\\nmodel is differentiable with respect to these parameters, this can be done efficiently\\nusing the hybrid Monte Carlo algorithm. This approach would allow the use of\\na prior distribution over high resolution pixel intensities which was confined to a\\nbounded interval, instead ofthe Gaussian assumed in this paper. Whether the additional improvements in performance will justify the extra computational complexity\\nremains to be seen.\\n\\nReferences\\n[1] N. Nguyen, P. Milanfar, and G. Golub. A computationally efficient superresolution\\nimage reconstruction algorithm. IEEE Transactions on Image Processing, 10(4):573583, 200l.\\n[2] V. N. Smelyanskiy, P. Cheeseman, D. Maluf, and R. Morris. Bayesian super-resolved\\nsurface reconstruction from images. In Proceedings CVPR, volume 1, pages 375- 382,\\n2000.\\n[3] D. P. Capel and A. Zisserman. Super-resolution enhancement of text image sequences.\\nIn International Conference on Pattern Recognition, pages 600- 605, Barcelona, 2000.\\n[4] R. C. Hardie, K. J. Barnard, and E. A. Armstrong. Joint MAP registration and\\nhigh-resolution image estimation using a sequence of undersampled images. IEEE\\nTransactions on Image Processing, 6(12):1621-1633, 1997.\\n[5] S. Baker and T. Kanade. Limits on super-resolution and how to break them. Technical\\nreport, Carnegie Mellon University, 2002. submitted to IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence.\\n[6] 1. T. Nabney. Netlab: Algorithms for Pattern Recognition. Springer, London, 2002.\\nhttp://www.ncrg.aston.ac. uk/netlab;'\\n[7] B. Bascle, A. Blake, and A. Zisserman. Motion deblurring and super-resolution from\\nan image sequence. In Proceedings of the Fourth European Conference on Computer\\nVision, pages 573- 581, Cambridge, England, 1996.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.dropna(subset=['paper_text'])"
      ],
      "metadata": {
        "id": "pHVwj2V_40He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwPN7Wo7z0ws",
        "outputId": "e6bcb7ee-7e9f-49a6-859a-0cfb47bd15a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25667, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75p3Vw8s5aXf",
        "outputId": "6f0d0601-374f-4dc6-bd2c-56c4982262a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id               0\n",
              "year             0\n",
              "title            0\n",
              "event_type    4778\n",
              "pdf_name         0\n",
              "abstract         0\n",
              "paper_text       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgDQNmt5f5p",
        "outputId": "9a9630b4-7936-458c-e347-5b6cfefab4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7217, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "x-2DK7W00FT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5gQX-UC0q1Y",
        "outputId": "e159ee36-1404-4fb3-bb3a-0ff8a29f2823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "XkohOS_e0HJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_defined_stopwords = ['fig', 'figure', 'result', 'using', 'show', 'large',\n",
        "                          'one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
        "                          'eight', 'nine', 'also']\n",
        "stop_words = list(stop_words.union(user_defined_stopwords))"
      ],
      "metadata": {
        "id": "4I9o-uHO0ouU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "jWSACNOV3e0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'<.*?>', ' ', text)\n",
        "  text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "  text = nltk.word_tokenize(text)\n",
        "  text = [word for word in text if word not in stop_words and len(word) > 3]\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "  return ' '.join(text)"
      ],
      "metadata": {
        "id": "i2HfBaA-1TEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df_cleaned['paper_text'].apply(lambda x: text_preprocessing(x))"
      ],
      "metadata": {
        "id": "vmWA84a22Pk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Vectorizer\n",
        "CountVectorizer in NLP is like a word counter. It converts text documents into a matrix where rows represent documents, columns represent unique words, and the cell values represent the frequency of each word. For example, given the documents \"I love NLP\" and \"NLP is amazing\", CountVectorizer would create a matrix with rows [1, 1] and [1, 1] for \"I love NLP\" and \"NLP is amazing\" respectively, with columns for each unique word and counts for occurrences.\n",
        "\n",
        "\n",
        "## TF-IDF Transformer\n",
        "TF-IDF Transformer in NLP transforms a count matrix from CountVectorizer into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features, which reflects the importance of each word in a document relative to its frequency across the corpus, thereby providing more meaningful features for text analysis and machine learning tasks. For example, given the count matrix [[1, 1], [1, 1]] from CountVectorizer, TF-IDF Transformer would compute TF-IDF scores for each word, considering both its frequency in the document and its rarity across the corpus."
      ],
      "metadata": {
        "id": "0jU0owMe7Aj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(max_df=0.95, max_features=5000, ngram_range=(1,3))\n",
        "word_count_vector = count_vectorizer.fit_transform(docs)"
      ],
      "metadata": {
        "id": "sbwfWNTK6wX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "transformer = transformer.fit(word_count_vector)"
      ],
      "metadata": {
        "id": "S7ZQJMnX8BLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword Extraction"
      ],
      "metadata": {
        "id": "sFf8SXMS8mPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = count_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "Js8-PzoF-LoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keywords(idx, doc, num_keywords=10):\n",
        "  doc_word_count = transformer.transform(count_vectorizer.transform([docs[idx]]))\n",
        "  doc_word_count = doc_word_count.tocoo()\n",
        "  tuples = zip(doc_word_count.col, doc_word_count.data)\n",
        "  sorted_tuples = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "  sorted_tuples = sorted_tuples[:num_keywords]\n",
        "\n",
        "  score_values = []\n",
        "  feature_values = []\n",
        "\n",
        "  for idx, score in sorted_tuples:\n",
        "    score_values.append(round(score, 3))\n",
        "    feature_values.append(feature_names[idx])\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  for idx in range(len(feature_values)):\n",
        "    results[feature_values[idx]] = score_values[idx]\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "rFRqXzbL9vBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_keywords(idx, keywords, df):\n",
        "  print('==========Title==========')\n",
        "  print(df['title'][idx])\n",
        "  print('==========Abstract==========')\n",
        "  print(df['abstract'][idx])\n",
        "  print('==========Keywords==========')\n",
        "  for keyword in keywords:\n",
        "    print(keyword, keywords[keyword])"
      ],
      "metadata": {
        "id": "t1XgeFkC8n1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(count_vectorizer, open('count_vectorizer.pkl', 'wb'))\n",
        "pickle.dump(transformer, open('transformer.pkl', 'wb'))\n",
        "pickle.dump(feature_names, open('feature_names.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "lFx-omBLAZaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}